{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. CONNECT TO GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "LBN5Hfgck_dT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nRMQbflL6Xh",
        "outputId": "0bf21cf4-a287-4044-d495-2b7226cad5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Connecting to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. IMPORTING DEPENDENDCIES AND MAKE PATHS TO THE FOLDERS"
      ],
      "metadata": {
        "id": "puXcZLvMlH1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import os\n",
        "\n",
        "#Directory to emotion dataset\n",
        "data_dir = os.path.join('/content/drive/MyDrive/emotion_det')\n",
        "\n",
        "# Path to the pre trained model YOLOv5\n",
        "path_YOLO_dir = os.path.join('/content/drive/MyDrive/yolov5')\n",
        "\n",
        "# Path to dataset (.yaml) - paths to dataset and class named dictionary\n",
        "path_dataset = os.path.join('/content/drive/MyDrive/emotion_dataset.yaml')\n",
        "\n",
        "\n",
        "# Paths to trained images\n",
        "angry_trainImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/train/angry')\n",
        "disgust_trainImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/train/disgust')\n",
        "fear_trainImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/train/fear')\n",
        "happy_trainImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/train/happy')\n",
        "neutral_trainImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/train/neutral')\n",
        "sad_trainImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/train/sad')\n",
        "surprise_trainImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/train/surprise')\n",
        "\n",
        "# Paths to trained labels\n",
        "angry_trainLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/train/angry')\n",
        "disgust_trainLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/train/disgust')\n",
        "fear_trainLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/train/fear')\n",
        "happy_trainLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/train/happy')\n",
        "neutral_trainLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/train/neutral')\n",
        "sad_trainLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/train/sad')\n",
        "surprise_trainLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/train/surprise')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Paths to validation images\n",
        "angry_valImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/test/angry')\n",
        "disgust_valImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/test/disgust')\n",
        "fear_valImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/test/fear')\n",
        "happy_valImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/test/happy')\n",
        "neutral_valImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/test/neutral')\n",
        "sad_valImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/test/sad')\n",
        "surprise_valImg_dir = os.path.join('/content/drive/MyDrive/emotion_det/test/surprise')\n",
        "\n",
        "\n",
        "# Paths to validation labels\n",
        "angry_valLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/test/angry')\n",
        "disgust_valLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/test/disgust')\n",
        "fear_valLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/test/fear')\n",
        "happy_valLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/test/happy')\n",
        "neutral_valLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/test/neutral')\n",
        "sad_valLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/test/sad')\n",
        "surprise_valLabel_dir = os.path.join('/content/drive/MyDrive/emotion_det/labels/test/surprise')\n",
        "\n"
      ],
      "metadata": {
        "id": "9B1J-JBqMbL4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. CHECK THE TRAINED IMAGES AND LABELS AND VERIFY THE PATH TO THE DATASET"
      ],
      "metadata": {
        "id": "OAw1ZJm0lSGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print of trained images - to see how many are in each folder\n",
        "print(f'total training angry images: {len(os.listdir(angry_trainImg_dir))}')\n",
        "print(f'total training disgust images: {len(os.listdir(disgust_trainImg_dir))}')\n",
        "print(f'total training fear images: {len(os.listdir(fear_trainImg_dir))}')\n",
        "print(f'total training happy images: {len(os.listdir(happy_trainImg_dir))}')\n",
        "print(f'total training neutral images: {len(os.listdir(neutral_trainImg_dir))}')\n",
        "print(f'total training sad images: {len(os.listdir(sad_trainImg_dir))}')\n",
        "print(f'total training surprise images: {len(os.listdir(surprise_trainImg_dir))}')\n",
        "\n",
        "\n",
        "\n",
        "# Print of trained labels - to see how many are in each folder\n",
        "print(f'total training angry labels: {len(os.listdir(angry_trainLabel_dir))}')\n",
        "print(f'total training disgust labels: {len(os.listdir(disgust_trainLabel_dir))}')\n",
        "print(f'total training fear labels: {len(os.listdir(fear_trainLabel_dir))}')\n",
        "print(f'total training happy labels: {len(os.listdir(happy_trainLabel_dir))}')\n",
        "print(f'total training neutral labels: {len(os.listdir(neutral_trainLabel_dir))}')\n",
        "print(f'total training sad labels: {len(os.listdir(sad_trainLabel_dir))}')\n",
        "print(f'total training surprise labels: {len(os.listdir(surprise_trainLabel_dir))}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4H3u1njCzKf",
        "outputId": "b95c14bd-2026-4a01-9bfc-6f0f5027cb07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training angry images: 4095\n",
            "total training disgust images: 436\n",
            "total training fear images: 4217\n",
            "total training happy images: 7225\n",
            "total training neutral images: 5085\n",
            "total training sad images: 4930\n",
            "total training surprise images: 364\n",
            "total training angry labels: 4022\n",
            "total training disgust labels: 438\n",
            "total training fear labels: 4114\n",
            "total training happy labels: 7263\n",
            "total training neutral labels: 4991\n",
            "total training sad labels: 4866\n",
            "total training surprise labels: 3183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. CHECK THE VALIDATION IMAGES AND LABELS AND VERIFY THE PATH TO THE DATASET"
      ],
      "metadata": {
        "id": "VzHN9XZdlf_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print of validation images - to see how many are in each folder\n",
        "print(f'total validation angry images: {len(os.listdir(angry_valImg_dir))}')\n",
        "print(f'total validation disgust images: {len(os.listdir(disgust_valImg_dir))}')\n",
        "print(f'total validation fear images: {len(os.listdir(fear_valImg_dir))}')\n",
        "print(f'total validation happy images: {len(os.listdir(happy_valImg_dir))}')\n",
        "print(f'total validation neutral images: {len(os.listdir(neutral_valImg_dir))}')\n",
        "print(f'total validation sad images: {len(os.listdir(sad_valImg_dir))}')\n",
        "print(f'total validation surprise images: {len(os.listdir(surprise_valImg_dir))}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print of validation labels - to see how many are in each folder\n",
        "print(f'total validation angry labels: {len(os.listdir(angry_valLabel_dir))}')\n",
        "print(f'total validation disgust labels: {len(os.listdir(disgust_valLabel_dir))}')\n",
        "print(f'total validation fear labels: {len(os.listdir(fear_valLabel_dir))}')\n",
        "print(f'total validation happy labels: {len(os.listdir(happy_valLabel_dir))}')\n",
        "print(f'total validation neutral labels: {len(os.listdir(neutral_valLabel_dir))}')\n",
        "print(f'total validation sad labels: {len(os.listdir(sad_valLabel_dir))}')\n",
        "print(f'total validation surprise labels: {len(os.listdir(surprise_valLabel_dir))}')\n",
        "\n",
        "#print(f'total validation labels: {len(os.listdir(val_label_dir))}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KXGtcuJHL-Q",
        "outputId": "c266d800-f30f-4022-99f6-f770cdca13cd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total validation angry images: 958\n",
            "total validation disgust images: 111\n",
            "total validation fear images: 1024\n",
            "total validation happy images: 1774\n",
            "total validation neutral images: 1233\n",
            "total validation sad images: 1247\n",
            "total validation surprise images: 831\n",
            "total validation angry labels: 233\n",
            "total validation disgust labels: 44\n",
            "total validation fear labels: 506\n",
            "total validation happy labels: 902\n",
            "total validation neutral labels: 589\n",
            "total validation sad labels: 633\n",
            "total validation surprise labels: 418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. TRAINING THE MODEL"
      ],
      "metadata": {
        "id": "dkA1VzMIlzUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using CUDA GPU for faster training\n",
        "import torch\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#Path to the weigths from the trained face detection model\n",
        "weights_path = os.path.join('/content/drive/MyDrive/Training/Training_logs_outputs6/weights/best.pt')\n",
        "\n",
        "\n",
        "# To acces the directory where the yolov5 file is located\n",
        "%cd \"{path_YOLO_dir}\"\n",
        "\n",
        "\n",
        "# Train the model with weights from the trained face detection model using YOLOv5s\n",
        "!python train.py \\\n",
        "--cfg \"/content/drive/MyDrive/yolov5/models/yolov5s.yaml\" \\\n",
        "--data \"{path_dataset}\" \\\n",
        "--epochs 10 \\\n",
        "--batch-size 32 \\\n",
        "--imgsz 64 \\\n",
        "--weights \"{weights_path}\" \\\n",
        "--device {device} \\\n",
        "--name \"Emotion_logs_outputs\" \\\n",
        "--project \"/content/drive/MyDrive/Trained_emotion_detection\"\n",
        "\n",
        "print(\"Training is complete.\")"
      ],
      "metadata": {
        "id": "qVCMsDEtHTej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039a1b48-7483-4a5d-ad67-fc567a80ad8f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "/content/drive/MyDrive/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "2024-12-11 22:17:20.904782: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-11 22:17:20.926496: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-11 22:17:20.932973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/drive/MyDrive/Training/Training_logs_outputs6/weights/best.pt, cfg=/content/drive/MyDrive/yolov5/models/yolov5s.yaml, data=/content/drive/MyDrive/emotion_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=32, imgsz=64, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cuda:0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/content/drive/MyDrive/Trained_emotion_detection, name=Emotion_logs_outputs, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 1 commit. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
            "YOLOv5 🚀 v7.0-388-g882c35fc Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive/Trained_emotion_detection', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=7\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7038508 parameters, 7038508 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 342/349 items from /content/drive/MyDrive/Training/Training_logs_outputs6/weights/best.pt\n",
            "/content/drive/MyDrive/yolov5/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/drive/MyDrive/yolov5/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/emotion_det/train/angry... 0 images, 26352 backgrounds, 0 corrupt: 100% 26352/26352 [01:56<00:00, 226.72it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ No labels found in /content/drive/MyDrive/emotion_det/train/angry.cache. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/emotion_det/train/angry.cache\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/yolov5/train.py\", line 986, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/drive/MyDrive/yolov5/train.py\", line 688, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/drive/MyDrive/yolov5/train.py\", line 285, in train\n",
            "    train_loader, dataset = create_dataloader(\n",
            "  File \"/content/drive/MyDrive/yolov5/utils/dataloaders.py\", line 184, in create_dataloader\n",
            "    dataset = LoadImagesAndLabels(\n",
            "  File \"/content/drive/MyDrive/yolov5/utils/dataloaders.py\", line 612, in __init__\n",
            "    assert nf > 0 or not augment, f\"{prefix}No labels found in {cache_path}, can not start training. {HELP_URL}\"\n",
            "AssertionError: \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in /content/drive/MyDrive/emotion_det/train/angry.cache, can not start training. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n",
            "Training is complete.\n"
          ]
        }
      ]
    }
  ]
}